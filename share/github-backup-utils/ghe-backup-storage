#!/usr/bin/env bash
#/ Usage: ghe-backup-storage
#/ Take an online, incremental snapshot of all Alambic Storage data using the
#/ calculated routes method.
#/
#/ Note: This command typically isn't called directly. It's invoked by
#/ ghe-backup.
set -e

# Bring in the backup configuration
. $( dirname "${BASH_SOURCE[0]}" )/ghe-backup-config

bm_start "$(basename $0)"

# Set up remote host and root backup snapshot directory based on config
host="$GHE_HOSTNAME"
backup_dir="$GHE_SNAPSHOT_DIR/storage"

# Verify rsync is available.
if ! rsync --version 1>/dev/null 2>&1; then
    echo "Error: rsync not found." 1>&2
    exit 1
fi

# Perform a host-check and establish GHE_REMOTE_XXX variables.
ghe_remote_version_required "$host"

# Split host:port into parts
port=$(ssh_port_part "$GHE_HOSTNAME")
host=$(ssh_host_part "$GHE_HOSTNAME")

# Add user / -l option
user="${host%@*}"
[ "$user" = "$host" ] && user="admin"

hostnames=$host
ssh_config_file_opt=
tempdir=$(mktemp -d -t backup-utils-restore-XXXXXX)
opts="$GHE_EXTRA_SSH_OPTS"

# storage server hostnames under cluster
if [ "$GHE_BACKUP_STRATEGY" = "cluster" ]; then
  ssh_config_file="$tempdir/ssh_config"
  ssh_config_file_opt="-F $ssh_config_file"
  opts="$opts -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o PasswordAuthentication=no"
  hostnames=$(ghe-cluster-nodes "$GHE_HOSTNAME" "storage-server")
  ghe-ssh-config "$GHE_HOSTNAME" "$hostnames" > "$ssh_config_file"
fi

# Make sure root backup dir exists if this is the first run
mkdir -p "$backup_dir"

# Removes the remote sync-in-progress file on exit, re-enabling GC operations
# on the remote instance.
cleanup() {
  # Enable remote maintenance operations
  for hostname in $hostnames; do
    ghe-gc-enable $ssh_config_file_opt $hostname:$port || true
  done

  ghe-ssh "$GHE_HOSTNAME" -- rm -rf $tempdir
  rm -rf $tempdir
}
trap 'cleanup' EXIT INT

# Disable remote maintenance operations
for hostname in $hostnames; do
  ghe-gc-disable $ssh_config_file_opt $hostname:$port
done

# If we have a previous increment and it is not empty, avoid transferring existing files via rsync's
# --link-dest support. This also decreases physical space usage considerably.
if [ -d "$GHE_DATA_DIR/current/storage" ] && [ "$(ls -A $GHE_DATA_DIR/current/storage)" ]; then
  link_dest="--link-dest=../../current/storage"
fi

# Calculate sync routes. This will store the healthy object paths for each node
#
# This gets a repo path and stores the path in the $node.sync file
# a/nw/a8/3f/02/100000855 storage-server-node1 >> storage-server-node1.sync
# a/nw/a8/bc/8d/100000880 storage-server-node3 >> storage-server-node3.sync
# a/nw/a5/06/81/100000659 storage-server-node2 >> storage-server-node2.sync
# ...
#one route per line.
#
bm_start "$(basename $0) - Calculating sync routes"
ghe-ssh "$GHE_HOSTNAME" github-env ./bin/storage-cluster-backup-routes \
  | while read route; do
  ghe_debug "Got backup route $route"
  if [ "$GHE_BACKUP_STRATEGY" = "cluster" ]; then
    server=$(echo $route | cut -d ' ' -f2-)
  else
    server=$host
  fi
  network_path=$(echo $route | cut -d ' ' -f1)
  ghe_debug "Adding $network_path to $tempdir/$server.rsync"
  echo "$network_path" >> $tempdir/$server.rsync
done
bm_end "$(basename $0) - Calculating sync routes"

if ! ls $tempdir/*.rsync >/dev/null 2>&1; then
  echo "Warning: no routes found, skipping storage backup ..."
  exit 0
fi

# rsync all the storage objects
bm_start "$(basename $0) - Storage object sync"
for file_list in $tempdir/*.rsync; do
  hostname=$(basename $file_list .rsync)

  object_num=$(cat $file_list | wc -l)
  ghe_verbose "* Transferring $object_num objects from $hostname"

  ghe-rsync -avr \
  -e "ssh -v $opts -p $port $ssh_config_file_opt -l $user" \
  $link_dest "$@" \
  --rsync-path='sudo -u git rsync' \
  --files-from="$file_list" \
  --size-only \
  "$hostname:$GHE_REMOTE_DATA_USER_DIR/storage/" \
  "$backup_dir" 1>&3 2>&3 &
done

for pid in $(jobs -p); do
  wait $pid
done
bm_end "$(basename $0) - Storage object sync"

bm_end "$(basename $0)"
